{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c155a90-0865-4ef1-b24d-939b60263db5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Limpeza de Ambiente"
    }
   },
   "outputs": [],
   "source": [
    "##### COMANDO SPARK PARA LIMPAR O CACHE DO NOTEBOOK\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaebf74d-1ec3-4af1-8ec0-18e4efbb0410",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Bibliotecas e Importações"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pytz\n",
    "\n",
    "from datetime import datetime\n",
    "from CommonsUtils import crawler_glue_catalog, databrickslog\n",
    "from delta.tables import DeltaTable\n",
    "from PIL import Image\n",
    "from pyspark.dbutils import DBUtils\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a34aae4-c7e2-4b2f-835a-ea670cdff3aa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Variáveis e Constantes"
    }
   },
   "outputs": [],
   "source": [
    "##### CONSTANTES E VARIAVEIS DE EXECUCAO\n",
    "#name_schema = 'exp_fluxo_de_caixa_pj'\n",
    "table_theme = \"fluxo-de-caixa-pj\"\n",
    "TABLE_TYPE = \"exp\"\n",
    "ACTION_VIEW = 'view'\n",
    "NUM_VERSION = \"Diana 1.5\"       ##Limpeza automática do cache do notebook de serviço\n",
    "#NUM_VERSION = \"Diana 1.4\"      ##Permissão para visualização de view no PBI e DENODO\n",
    "#NUM_VERSION = \"Diana 1.3\"      ##Controle de logs de processamento\n",
    "\n",
    "\n",
    "##### VARIAVEIS OBTIDAS EM SISTEMA\n",
    "catalog_service = None\n",
    "catalog_linked = None\n",
    "default_bucket = None\n",
    "environment = None\n",
    "url_dbk_ = None\n",
    "name_cluster = None\n",
    "name_tabela = None\n",
    "table_columns = None\n",
    "table_pk = None\n",
    "table_partition = None\n",
    "table_comment = None\n",
    "source_view = None\n",
    "check_table_active = None\n",
    "status_validacao = \"Não validado\"\n",
    "\n",
    "\n",
    "##### AMBIENTE DEV/PRD\n",
    "default_bucket = os.getenv(\"DEFAULT_BUCKET\")\n",
    "environment = default_bucket.split('-')[2]\n",
    "\n",
    "\n",
    "##### URL AMBIENTE\n",
    "url_dbk_ = spark.conf.get('spark.databricks.workspaceUrl')\n",
    "if 'DEV' in url_dbk_.upper():\n",
    "    ambinete_dbk_ = 'Dev'\n",
    "else:\n",
    "    ambinete_dbk_ = 'Prd'\n",
    "\n",
    "\n",
    "##### SELECAO CLUSTER + CATALOGO\n",
    "catalog_cluster = spark.conf.get(\"spark.databricks.sql.initial.catalog.name\").replace(\"_dev\", \"\")\n",
    "catalog_linked = \"banco_cooperativo_cas\"\n",
    "spark.conf.set(f\"spark.databricks.sql.initial.catalog\", catalog_linked)\n",
    "\n",
    "\n",
    "##### AJUSTANDO TIME ZONE\n",
    "#spark_ = SparkSession.builder.getOrCreate()\n",
    "#spark.conf.set(\"spark.sql.session.timeZone\", \"America/Sao_Paulo\")\n",
    "#spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "#dbutils = DBUtils(spark_)\n",
    "\n",
    "\n",
    "##### VARIAVEIS DE TEMPO DE EXECUCAO\n",
    "time_start = int(datetime.now().strftime('%H%M%S'))\n",
    "current_date = datetime.now(pytz.timezone('America/Sao_Paulo')).strftime('%d-%m-%Y')\n",
    "current_time_date = datetime.now(pytz.timezone('America/Sao_Paulo')).strftime('%d-%m-%Y %H:%M:%S')\n",
    "\n",
    "\n",
    "##### VARIAVEIS DOS OBJETOS PARALELOS\n",
    "if environment == 'dev':\n",
    "    current_user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "    current_user = current_user.split('@', 1)[0]\n",
    "else:\n",
    "    current_user = \"/e-mail_usuario\"\n",
    "\n",
    "note_documentacao = (f\"https://banco_cooperativo-dev.cloud.databricks.com/?o=#workspace{current_user}@banco_cooperativo.com.br/repositorio_exps/utils/Diana_framework/Documentação_Diana_framework\")\n",
    "Note_modelo_tabela = (f\"https://banco_cooperativo-dev.cloud.databricks.com/?o=#workspace{current_user}@banco_cooperativo.com.br/repositorio_exps/utils/Diana_framework/Note_modelo_tabela\")\n",
    "Note_modelo_tabela_python = (f\"https://banco_cooperativo-dev.cloud.databricks.com/?o=#workspace{current_user}@banco_cooperativo.com.br/repositorio_exps/utils/Diana_framework/Note_modelo_tabela_python\")\n",
    "Note_modelo_view = (f\"https://banco_cooperativo-dev.cloud.databricks.com/?o=#workspace{current_user}@banco_cooperativo.com.br/repositorio_exps/utils/Diana_framework/Note_modelo_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37f27bd4-f104-40f2-9392-df4bdc8caa78",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Variáveis do DEF"
    }
   },
   "outputs": [],
   "source": [
    "class ComandVariavel:\n",
    "\n",
    "    ## COMANDO PARA ATUALIZAR VARIAVEIS - TABLE\n",
    "    def _atualiza_variavel():\n",
    "\n",
    "        global name_tabela\n",
    "        global table_columns\n",
    "        global table_pk\n",
    "        global table_partition\n",
    "        global table_comment\n",
    "        global source_view\n",
    "        global name_schema\n",
    "        \n",
    "        name_tabela = TableManager.source_view.lower()\n",
    "        name_schema = TableManager.schema_name.lower()\n",
    "        TableManager.table_name = TableManager.source_view\n",
    "        table_pk = TableManager.table_pk\n",
    "        table_columns = TableManager.table_columns\n",
    "        table_partition = (\n",
    "            [col.upper() for col in TableManager.table_partition]\n",
    "            if hasattr(TableManager, 'table_partition') and TableManager.table_partition \n",
    "            else None\n",
    "        )\n",
    "\n",
    "\n",
    "    ## COMANDO PARA ATUALIZAR VARIAVEIS - VIEWA\n",
    "    def _atualiza_view():\n",
    "\n",
    "        global name_schema\n",
    "        global query_view\n",
    "        global name_view\n",
    "\n",
    "        name_view = \"vw_\" + TableManager.name_view.lower()\n",
    "        name_schema = TableManager.schema_name.lower()\n",
    "        query_view = TableManager.query_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "386c31ab-0a7e-4e60-b645-65c35df06e87",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Calculo Tempo de Procesamento"
    }
   },
   "outputs": [],
   "source": [
    "class ComandTime:\n",
    "\n",
    "    ## COMANDO PARA CRIAR A TABELA DE SALVAMENTO DOS LOGS EM CASO DE ERRO\n",
    "    object_log = spark.catalog.tableExists(\"banco_cooperativo_CAS.EXP_FLUXO_DE_CAIXA_PJ.FRAME_PROCESS_DETAILS\")\n",
    "\n",
    "    if object_log is False:\n",
    "        df_time_process = spark.createDataFrame([], \n",
    "                                        \"VERSAO_FRAMEWORK string \\\n",
    "                                            , OBJETO_VALIDACAO string \\\n",
    "                                            , OBJETO_ESCRITA string \\\n",
    "                                            , OBJETO_SCHEMA string \\\n",
    "                                            , OBJETO_NOME string \\\n",
    "                                            , DATA_EXECUCAO string \\\n",
    "                                            , TEMPO_PROCESSAMENTO string \\\n",
    "                                            , ETL_CREATED_ON string \\\n",
    "                                            , ETL_UPDATED_ON string, UNIQUE_ID string\")\n",
    "\n",
    "        df_time_process.write.format(\"delta\") \\\n",
    "                    .mode(\"overwrite\") \\\n",
    "                    .saveAsTable(f\"banco_cooperativo_CAS.EXP_FLUXO_DE_CAIXA_PJ.FRAME_PROCESS_DETAILS\")\n",
    "        pass\n",
    "\n",
    "\n",
    "    ## COMANDO QUE CALCULA O TEMPO TOTAL DE PROCESSAMENTO\n",
    "    def calculed_time():\n",
    "        if environment == 'prd':\n",
    "\n",
    "            time_end = int(datetime.now().strftime(\"%H%M%S\"))\n",
    "            time_full = time_end - time_start\n",
    "\n",
    "            ## FAZ A CONVERSAO DO TEMPO  DE PROCESSAMENTO\n",
    "            hours = time_full // 3600\n",
    "            minutes = (time_full % 3600) // 60\n",
    "            seconds = time_full % 60\n",
    "            time_process = f\"{hours:02d}:{minutes:02d}:{seconds:02d}\"\n",
    "\n",
    "            ## FAZ A INSERSAO DOS LOGS DO OBJETO\n",
    "            df_insert = spark.createDataFrame([(\n",
    "                    f\"{NUM_VERSION}\",\n",
    "                    f\"{status_validacao}\",\n",
    "                    f\"{tipo_escrita}\",\n",
    "                    f\"{name_schema}\",\n",
    "                    f\"{name_tabela}\",\n",
    "                    f\"{current_date}\",\n",
    "                    f\"{time_process}\",\n",
    "                    f\"{current_time_date}\",\n",
    "                    \"\",\n",
    "                    \"\",\n",
    "                )],\n",
    "                [\n",
    "                    \"VERSAO_FRAMEWORK\",\n",
    "                    \"OBJETO_VALIDACAO\",\n",
    "                    \"OBJETO_ESCRITA\",\n",
    "                    \"OBJETO_SCHEMA\",\n",
    "                    \"OBJETO_NOME\",\n",
    "                    \"DATA_EXECUCAO\",\n",
    "                    \"TEMPO_PROCESSAMENTO\",\n",
    "                    \"ETL_CREATED_ON\",\n",
    "                    \"ETL_UPDATED_ON\",\n",
    "                    \"UNIQUE_ID\",\n",
    "                ])\n",
    "\n",
    "            ## CRIA O MASCARAMENTO DA CHAVE DO LOG\n",
    "            pk_linha = [\"OBJETO_NOME\", \"DATA_EXECUCAO\", \"ETL_CREATED_ON\"]\n",
    "            df_insert = df_insert.withColumn(\"UNIQUE_ID\", md5(concat_ws(\":\", *pk_linha)))\n",
    "\n",
    "            df_insert.write.format(\"delta\").mode(\"append\").saveAsTable(\"banco_cooperativo_CAS.EXP_FLUXO_DE_CAIXA_PJ.FRAME_PROCESS_DETAILS\")\n",
    "            print(f\"TEMPO DE PROCESSAMENTO: {time_process}\")\n",
    "\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b44129f-43c7-4174-97cc-b50daa881cec",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Conexões de Serviço"
    }
   },
   "outputs": [],
   "source": [
    "class FrameConection:\n",
    "\n",
    "    ## COMANDO PARA CRIAR CONEXAO TERADATA\n",
    "    @staticmethod\n",
    "    def get_connect_teradata(query, user=None, password=None, scope=None):\n",
    "        user = 'username_teradata_prd'\n",
    "        password = 'password_teradata_prd'\n",
    "        scope = 'databricks-jdbc-teradata-fluxo_de_caixa'\n",
    "\n",
    "        driver_r = {\"teradata\": \"com.teradata.jdbc.TeraDriver\"}\n",
    "        url_r = {\"teradata\": \"jdbc:teradata://teradbaws.banco_cooperativo.net/Database=dbc,CHARSET=UTF8,TMODE=TERA,LOGMECH=LDAP\"}\n",
    "\n",
    "        def get_secret(scope, key):\n",
    "            return dbutils.secrets.get(scope, key)\n",
    "        \n",
    "        df_jdbc = spark.read.format(\"jdbc\") \\\n",
    "              .option(\"driver\", driver_r['teradata']) \\\n",
    "              .option(\"url\", url_r['teradata']) \\\n",
    "              .option(\"query\", query) \\\n",
    "              .option(\"user\", get_secret(scope, user)) \\\n",
    "              .option(\"password\", get_secret(scope, password)) \\\n",
    "              .option(\"numPartitions\", 12) \\\n",
    "              .load()\n",
    "        return df_jdbc\n",
    "    \n",
    "\n",
    "    ## COMANDO PARA CRIAR CONEXAO DENODO \n",
    "    @staticmethod\n",
    "    def databricks_denodo_connection(query_dnd, user_dnd=None, password_dnd=None, scope_dnd=None): \n",
    "        user_dnd = dbutils.secrets.get('databricks-jdbc-denodo-fluxo_de_caixa', 'fluxo_denodo_user')\n",
    "        password_dnd = dbutils.secrets.get('databricks-jdbc-denodo-fluxo_de_caixa', 'fluxo_denodo_passw')\n",
    "        scope_dnd = \"ldw\"\n",
    "        \n",
    "        df_dnd_jdbc = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", f\"jdbc:vdb://virtualizador.banco_cooperativo.net:9999/{scope_dnd}\") \\\n",
    "            .option(\"driver\", \"com.denodo.vdp.jdbc.Driver\") \\\n",
    "            .option(\"user\", user_dnd) \\\n",
    "            .option(\"password\", password_dnd) \\\n",
    "            .option(\"query\", f\"({query_dnd})\") \\\n",
    "            .option(\"numPartitions\", 2) \\\n",
    "            .load()\n",
    "        return df_dnd_jdbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e01621ac-8a5b-4a40-a6df-e423c2a45fbc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Comando - Metadados Table + View"
    }
   },
   "outputs": [],
   "source": [
    "class ComandMetadados:\n",
    "\n",
    "    ## COMANDO PARA CRIAR OS METADADOS DO OBJETO\n",
    "    def _atualiza_metadados():\n",
    "        ComandVariavel._atualiza_variavel()\n",
    "        df = spark.table(TableManager.source_view)\n",
    "        for col_name, col_type, col_comment in TableManager.table_columns:\n",
    "            if col_name.upper() in df.columns:\n",
    "                spark.sql(f\"ALTER TABLE {catalog_linked}.{name_schema}.{name_tabela} CHANGE {col_name} COMMENT '{col_comment}'\")\n",
    "\n",
    "        spark.sql(f\"ALTER TABLE {catalog_linked}.{name_schema}.{name_tabela} CHANGE ETL_CREATED_ON COMMENT 'Data de criação do registro'\")\n",
    "        spark.sql(f\"ALTER TABLE {catalog_linked}.{name_schema}.{name_tabela} CHANGE ETL_UPDATED_ON COMMENT 'Data de atualização do registro'\")\n",
    "        spark.sql(f\"ALTER TABLE {catalog_linked}.{name_schema}.{name_tabela} CHANGE UNIQUE_ID COMMENT 'Identificador único do registro'\")\n",
    "\n",
    "\n",
    "    ## COMANDO PRA CRIAR O SYNC DO UNITY CATALOG - TABLE\n",
    "    def _sync_unity_catalog(): \n",
    "        func = crawler_glue_catalog.GlueUtils\n",
    "\n",
    "        ## CRIA CRAWLER TABLE\n",
    "        try:\n",
    "            func.glue_sync_table(f'{catalog_linked}', f'{name_schema}', f'{name_tabela}', 'create', True)\n",
    "            print(\"\\033[38;5;40m➥ Metadados criados via Athena \\033[0m\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\033[91m✖ Erro ao criar metadados via Athena: {str(e)}...\\033[0m\")\n",
    "\n",
    "\n",
    "    ## COMANDO PRA CRIAR O SYNC DO UNITY CATALOG - VIEW\n",
    "    def _sync_unity_catalog_view():\n",
    "        ComandVariavel._atualiza_view()\n",
    "        func = crawler_glue_catalog.GlueUtils\n",
    "        \n",
    "        ## CRIA CRAWLER VIEW\n",
    "        try:\n",
    "            view_schema = func.get_dict_schema_table(catalog_linked, name_schema, name_view)\n",
    "            view_schema = str(view_schema).replace('timestamp_ntz', 'timestamp')\n",
    "            view_schema = eval(view_schema)\n",
    "            func.glue_sync_table(catalog_linked, name_schema, name_view, ACTION_VIEW, False, TableManager.query_view, view_schema)\n",
    "            print(\"\\033[38;5;40m➥ Metadados criados via Athena \\033[0m\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09779d40-56db-4732-af54-002a20fa8363",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Comando - Check Table"
    }
   },
   "outputs": [],
   "source": [
    "class ComandCheck:\n",
    "\n",
    "    ## COMANDO PARA VERIFICAR SE A TABELA ATENDE O MINIMO ESPERADO\n",
    "    def _check_table():\n",
    "\n",
    "        global check_table_active\n",
    "        global status_validacao\n",
    "\n",
    "        !clear\n",
    "        ComandVariavel._atualiza_variavel()\n",
    "\n",
    "        list_source_view = ([c.name for c in spark.read.table(TableManager.source_view).schema.fields])\n",
    "        num_col_source_view = len(list_source_view)\n",
    "\n",
    "        list_table_columns = ([c[0] for c in TableManager.table_columns])\n",
    "        num_col_table_columns = len(list_table_columns)\n",
    "\n",
    "        ## VALIDA SE EXITE OS WIDGETS INFORMATIVOS NO PROJETO\n",
    "        try:\n",
    "            wid_bu = dbutils.widgets.get(\"BU_Responsavel\")\n",
    "            wid_atualizacao = dbutils.widgets.get(\"Periodo_Atualizacao\")\n",
    "            wid_plataforma = dbutils.widgets.get(\"Plataforma\")\n",
    "            wid_lista = [wid_bu, wid_atualizacao, wid_plataforma]\n",
    "\n",
    "            ## VALIDA SE OS WIDGETS INFORMATIVOS FORAM PREENCHIDOS CORRETAMENTE\n",
    "            if '' not in wid_lista:\n",
    "\n",
    "                ## VALIDA SE EXISTE O MESMO NUMERO DE COLUNAS NOS OBJETOS\n",
    "                if num_col_source_view == num_col_table_columns:                \n",
    "                    numero_col = [len(col) for col in TableManager.table_columns]\n",
    "\n",
    "                    ## VALIDA SE O NOME DAS COLUNAS SAO IGUAIS\n",
    "                    if [c.upper() for c in list_source_view] == [c.upper() for c in list_table_columns]:\n",
    "\n",
    "                        ## VALIDA SE O TABLE_COLUMNS TEM AS TRES COLUNAS DE FORMATACAO(NOME, FORMATO, DESCRICAO)\n",
    "                        if all(col == 3 for col in numero_col):\n",
    "\n",
    "                            ## VALIDA SE EXISTE CHAVE PK\n",
    "                            if table_pk:\n",
    "                                print(f\"O objeto foi armazenado no schema {name_schema}\")\n",
    "                                print(f\"O objeto final será nomeado como {name_tabela}\")\n",
    "                                check_table_active = \"Aprovado\"\n",
    "                                status_validacao = \"Aprovado\"\n",
    "\n",
    "                                ## VALIDA SE EXISTE PARTICIONAMENTO(NAO IMPEDE A PUBLICACAO)\n",
    "                                if table_partition:\n",
    "                                    print(f\"O objeto foi particionado pela coluna {table_partition}\")\n",
    "                                else:\n",
    "                                    print(f\"O objeto não foi particionado por decisão do usuário final\")\n",
    "                                print(\"\\033[38;5;40m➥ O objeto final foi validado pelo framework Diana, ela aceitou carregar o seu objeto ióóóó ióóóó \\033[0m\")\n",
    "                                \n",
    "                            else:\n",
    "                                check_table_active = \"Reprovado\"\n",
    "                                status_validacao = \"Reprovado\"\n",
    "                                print(\"\\033[93m⚠ O comando 'TableManager.table_pk' apresentou erro por estar com o campo vazio, favor verificar \\033[0m\")\n",
    "\n",
    "                        else:\n",
    "                            check_table_active = \"Reprovado\"\n",
    "                            status_validacao = \"Reprovado\"\n",
    "                            print(f\"\\033[93m⚠ A linha {numero_col.index(2) + 1} do comando TableManager.table_columns não apresenta a formatação correta, favor verificar \\033[0m\")\n",
    "\n",
    "                    else:\n",
    "                        check_table_active = \"Reprovado\"\n",
    "                        status_validacao = \"Reprovado\"\n",
    "\n",
    "                        ## VALIDA SE TODAS AS COLUNAS DE TABLE_COLUMNS ESTAO EM SOURCE_VIEW\n",
    "                        col_present_column = [col for col in list_table_columns if col.upper() not in [src_col.upper() for src_col in list_source_view]]\n",
    "                        if len(col_present_column) == 0:\n",
    "\n",
    "                            ## VALIDA SE A ORDEM DAS COLUNAS ESTA CORRETA\n",
    "                            col_diff_order = [col for col, src_col in zip(list_table_columns, list_source_view) if col.upper() != src_col.upper()]\n",
    "                            if col_diff_order == 1:\n",
    "                                print(f\"\\033[93m⚠ A coluna {col_diff_order[0]} citada no comando Table_Columns não está posicionada na mesma sequência da query final, favor verificar \\033[0m\")\n",
    "                            else:\n",
    "                                print(f\"\\033[93m⚠ As colunas {col_diff_order} citadas no comando Table_Columns não estão posicionadas na mesma sequência da query final, favor verificar \\033[0m\")\n",
    "\n",
    "                        elif len(col_present_column) == 1:\n",
    "                            print(f\"\\033[93m⚠  A coluna {col_present_column} citada no comando Table_Columns não está sendo citada na query final, favor verificar \\033[0m\")\n",
    "\n",
    "                        else:\n",
    "                            print(f\"\\033[93m⚠  As colunas {col_present_column} citadas no comando Table_Columns não estão sendo citadas na query final, favor verificar \\033[0m\")\n",
    "                        \n",
    "                else:\n",
    "                    check_table_active = \"Reprovado\"\n",
    "                    status_validacao = \"Reprovado\"\n",
    "\n",
    "                    ## VALIDA SE EXISTE COLUNAS COM DUPLICIDADE EM TABLE_COLUMNS\n",
    "                    duplicates_table_columns = [item for item in list_table_columns if list_table_columns.count(item) > 1]\n",
    "                    duplicates_table_columns = list(set(duplicates_table_columns))\n",
    "                    if len(duplicates_table_columns) == 0:\n",
    "                        pass\n",
    "                    elif len(duplicates_table_columns) == 1:\n",
    "                        print(f\"\\033[93m⚠ A coluna {duplicates_table_columns[0]} está duplicada no comando Table_Columns, favor verificar\\033[0m\")\n",
    "                    else:\n",
    "                        print(f\"\\033[93m⚠ As colunas {duplicates_table_columns} estão duplicadas no comando Table_Columns, favor verificar\\033[0m\")\n",
    "\n",
    "                    ## VALIDA ONDE ESTA A DIFRENÇA ENTRE AS LISTAS TABLE_COLUMNS E SOURCE_VIEW\n",
    "                    diff_list_source_view = [x.upper() for x in list_source_view if x.upper() not in [y.upper() for y in list_table_columns]]\n",
    "                    count_source_view = len(diff_list_source_view)\n",
    "\n",
    "                    diff_table_columns = [x.upper() for x in list_table_columns if x.upper() not in [y.upper() for y in list_source_view]]\n",
    "                    count_table_columns = len(diff_table_columns)\n",
    "\n",
    "                    if count_source_view == 0:\n",
    "                        pass\n",
    "                    elif count_source_view == 1:\n",
    "                        print(f\"\\033[93m⚠ A coluna {diff_list_source_view[0]} existe em {name_tabela} mas não foram declarada em TableManager.table_columns\\033[0m\")\n",
    "                    else:\n",
    "                        print(f\"\\033[93m⚠ As colunas {diff_list_source_view} existem em {name_tabela} mas não foram declaradas em TableManager.table_columns\\033[0m\")\n",
    "\n",
    "                    if count_table_columns == 0:\n",
    "                        pass\n",
    "                    elif count_table_columns == 1:\n",
    "                        print(f\"\\033[93m⚠ A coluna {diff_table_columns[0]} foi declarada em TableManager.table_columns mas não existe em {name_tabela}\\033[0m\")\n",
    "                    else:\n",
    "                        print(f\"\\033[93m⚠ As colunas {diff_table_columns} foram declaradas em TableManager.table_columns mas não existem em {name_tabela}\\033[0m\")\n",
    "\n",
    "            else:\n",
    "                check_table_active = \"Reprovado\"\n",
    "                status_validacao = \"Reprovado\"\n",
    "                print(f\"\\033[93m⚠ Todos os widgets no topo deste projeto devem ser preenchidos, favor verificar\\033[0m\")\n",
    "\n",
    "        ## COMANDO QUE ESTA NESTA POSICAO PARA FORCAR O USO DO NOTEBOOK MODELO       \n",
    "        except Exception as e:\n",
    "            check_table_active = \"Reprovado\"\n",
    "            status_validacao = \"Reprovado\"\n",
    "            if environment == 'dev':\n",
    "                print(f\"\\033[93m⚠ O projeto atual não contém widgets informativos, favor clonar o notebook modelo citado na célula de importação do framework Diana\\033[0m\")\n",
    "            pass        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77e9d487-d3f1-4a1c-b7a3-9e6585fc69bd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Comando - Colunas de controle"
    }
   },
   "outputs": [],
   "source": [
    "class ComandColumnsAdd:\n",
    "\n",
    "    ## COMANDO PARA ADICIONAR COLUNAS DE CONTROLE A TABELA FINAL\n",
    "    def _add_columns():\n",
    "        ComandVariavel._atualiza_variavel()\n",
    "        df = spark.table(TableManager.source_view)\n",
    "        df = df.toDF(*[c.upper() for c in df.columns])\n",
    "     \n",
    "        if 'ETL_CREATED_ON' not in df.columns:\n",
    "            df = df.withColumn('ETL_CREATED_ON', current_timestamp())\n",
    "\n",
    "        if 'ETL_UPDATED_ON' not in df.columns:\n",
    "            df = df.withColumn('ETL_UPDATED_ON', current_timestamp())\n",
    "\n",
    "        if 'UNIQUE_ID' not in df.columns:\n",
    "            pk_columns = [col(c) for c in table_pk]\n",
    "            df = df.withColumn('UNIQUE_ID', md5(concat_ws(':', *pk_columns)))\n",
    "        df.createOrReplaceTempView(TableManager.source_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bef617b5-80d7-4c92-82a0-54bd78b58d34",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Comando - Drop Table"
    }
   },
   "outputs": [],
   "source": [
    "class ComandDrop:\n",
    "\n",
    "    ## COMANDO PARA DROPAR TABELA \n",
    "    def _drop_table():\n",
    "        ComandVariavel._atualiza_variavel()\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {catalog_linked}.{name_schema}.{name_tabela}\")\n",
    "        #spark.sql(f\"DROP TABLE IF EXISTS {name_schema}.{name_tabela}\" + \"_delta\")\n",
    "        dbutils.fs.rm(f\"s3://{default_bucket}/{TABLE_TYPE}/{table_theme}/{name_tabela}\", recurse=True)\n",
    "        dbutils.fs.rm(f\"s3://{default_bucket}/{TABLE_TYPE}/{table_theme}/{name_tabela}_delta\", recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5015307-8292-40a2-b53b-8c8322ccef6c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Comando - Create Table Overwrite"
    }
   },
   "outputs": [],
   "source": [
    "class ComandCreateOver:\n",
    "\n",
    "    ## COMANDO PARA CRIAR TABELA EM MODO OVERWRITE\n",
    "    def _create_over_table():\n",
    "        global tipo_escrita\n",
    "        tipo_escrita = \"Overwrite\"\n",
    "        ComandVariavel._atualiza_variavel()\n",
    "        df_to_overwrite = spark.table(TableManager.source_view)\n",
    "        #spark.sql(f\"DROP TABLE IF EXISTS {name_schema}.{name_tabela}\")\n",
    "        #spark.sql(f\"DROP TABLE IF EXISTS {name_schema}.{name_tabela}\" + \"_delta\")\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {catalog_linked}.{name_schema}.{name_tabela}\")\n",
    "        spark.sql(f\"set spark.databricks.delta.properties.defaults.autoOptimize.optimizeWrite = true\")\n",
    "        spark.sql(f\"set spark.databricks.delta.properties.defaults.autoOptimize.autoCompact = true\")\n",
    "        df_to_overwrite.write.format(\"delta\") \\\n",
    "                                .option(\"comment\", TableManager.table_comment) \\\n",
    "                                .partitionBy(table_partition if table_partition is not None else []) \\\n",
    "                                .mode(\"overwrite\") \\\n",
    "                                .saveAsTable(f\"{catalog_linked}.{name_schema}.{name_tabela}\")\n",
    "        print(f\"\\033[38;5;40m➥ Tabela {catalog_linked}.{name_schema}.{name_tabela} criada com sucesso em modo OVERWRITE \\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49d3737d-1afd-4123-aa75-a4db8611e011",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Comando - Create Table Append"
    }
   },
   "outputs": [],
   "source": [
    "class ComandCreateAppe:\n",
    "\n",
    "    ## COMANDO PARA CRIAR TABELA EM MODO APPEND \n",
    "    def _create_appe_table():\n",
    "        global tipo_escrita\n",
    "        tipo_escrita = \"Append\"\n",
    "        ComandVariavel._atualiza_variavel()\n",
    "        df = spark.table(TableManager.source_view)\n",
    "        if spark._jsparkSession.catalog().tableExists(f\"{catalog_linked}.{name_schema}.{name_tabela}\"):\n",
    "            final_table = spark.table(f\"{catalog_linked}.{name_schema}.{name_tabela}\")\n",
    "            if final_table.count() > 0:\n",
    "                df_to_append = df.join(final_table, df.UNIQUE_ID == final_table.UNIQUE_ID, \"left_anti\")\n",
    "                df_to_append.write.format(\"delta\") \\\n",
    "                                    .option(\"comment\", TableManager.table_comment) \\\n",
    "                                    .partitionBy(table_partition if table_partition is not None else []) \\\n",
    "                                    .mode(\"append\") \\\n",
    "                                    .saveAsTable(f\"{catalog_linked}.{name_schema}.{name_tabela}\")\n",
    "                print(f\"\\033[38;5;40m➥ Tabela {catalog_linked}.{name_schema}.{name_tabela} incrementada com sucesso em modo APPEND \\033[0m\")\n",
    "            else:\n",
    "                pass           \n",
    "        else:\n",
    "            print(\"\"\"\\033[93m⚠ ATENÇÃO - O usuário selecionou o modo de escrita APPEND, mas a tabela informada não foi encontrada. O modo de escrita OVERWRITE será executado\\033[0m\"\"\")\n",
    "            ComandColumnsAdd._add_columns()\n",
    "            ComandCreateOver._create_over_table()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e737aaed-3c56-4412-873c-aa3512be35f3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Comando - Upsert Table"
    }
   },
   "outputs": [],
   "source": [
    "class ComandUpsert:\n",
    "\n",
    "    ## COMANDO PARA CRIAR TABELA EM MODO UPSERT (UPDATE + INSERT)\n",
    "    def _create_upsert_table():\n",
    "        global tipo_escrita\n",
    "        tipo_escrita = \"Upsert\"\n",
    "        ComandVariavel._atualiza_variavel()\n",
    "        if spark._jsparkSession.catalog().tableExists(f\"{catalog_linked}.{name_schema}.{name_tabela}\"):\n",
    "            final_table = spark.table(f\"{catalog_linked}.{name_schema}.{name_tabela}\")\n",
    "            columns = final_table.columns\n",
    "            final_table_str = ', '.join(f\"ft.{column}\" for column in columns)\n",
    "            final_table.createOrReplaceTempView(\"final_table\")\n",
    "            if final_table.count() > 0:\n",
    "                view_upsert = spark.table(TableManager.source_view)\n",
    "                columns = view_upsert.columns\n",
    "                view_upsert_str = ', '.join(f\"sv.{column}\" for column in columns)\n",
    "                view_upsert.createOrReplaceTempView(\"upsert_table\")\n",
    "                upsert_columns_ft = [f\"ft.{col}\" for col in final_table.columns]\n",
    "                upsert_columns_sv = [f\"sv.{col}\" for col in view_upsert.columns]\n",
    "                df_to_upsert = spark.sql(f\"\"\"\n",
    "                    MERGE INTO {catalog_linked}.{name_schema}.{name_tabela} AS ft\n",
    "                    USING upsert_table AS sv\n",
    "                        ON ft.UNIQUE_ID = sv.UNIQUE_ID\n",
    "                    WHEN MATCHED THEN\n",
    "                    UPDATE SET \n",
    "                        {', '.join(f\"{col_ft} = {col_sv}\" for col_ft, col_sv in zip(upsert_columns_ft, upsert_columns_sv))}\n",
    "                    WHEN NOT MATCHED THEN \n",
    "                        INSERT \n",
    "                            ({final_table_str})\n",
    "                        VALUES \n",
    "                            ({view_upsert_str})\n",
    "                \"\"\")\n",
    "                print(f\"\\033[38;5;40m➥ Tabela {catalog_linked}.{name_schema}.{name_tabela} incrementada com sucesso em modo UPSERT\\033[0m\")\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            print(\"\"\"\\033[93m⚠ ATENÇÃO - O usuário selecionou o modo de escrita UPSERT, mas a tabela informada não foi encontrada. O modo de escrita OVERWRITE será executado\\033[0m\"\"\")\n",
    "            ComandColumnsAdd._add_columns()\n",
    "            ComandCreateOver._create_over_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "247b43ca-caaf-4532-bd0b-f1151ad8f097",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Comando - Create View"
    }
   },
   "outputs": [],
   "source": [
    "class ComandCreateView:\n",
    "\n",
    "    ## COMANDOS PARA CRIAR UM TABELA EM MODO VIEW\n",
    "    def _create_view_table():\n",
    "        ComandVariavel._atualiza_view()\n",
    "        spark.sql(f\"DROP VIEW IF EXISTS {catalog_linked}.{name_schema}.{name_view}\")\n",
    "        dbutils.fs.rm(f\"s3://{default_bucket}/{TABLE_TYPE}/{table_theme}/{name_view}\", recurse=True)\n",
    "\n",
    "        ## CRIA VIEW\n",
    "        try:\n",
    "            spark.sql(f\"\"\"\n",
    "                CREATE OR REPLACE VIEW \n",
    "                    {catalog_linked}.{name_schema}.{name_view}\n",
    "                AS\n",
    "                    {query_view}\n",
    "            \"\"\")\n",
    "            print(f\"\\033[38;5;40m➥ A view {catalog_linked}.{name_schema}.{name_view} foi criada com sucesso\\033[0m\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            if \"because it references to the temporary object\" in str(e):\n",
    "                print(\"\\033[91m✖ A view solicitada não foi criada. A sua consulta tem dados temporários, favor utilizar somente REF's e EXP's em sua consulta\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb025cc6-5f20-4de9-a47d-4a8d86d27ea8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Comando - Display Table + View"
    }
   },
   "outputs": [],
   "source": [
    "class ComandDisplay:\n",
    "\n",
    "    ## COMANDO DISPLAY TABLE - AS PRIMEIRAS 2 LINHAS\n",
    "    def _display_table():\n",
    "        print(\"\\033[38;5;40m➥ A mula Diana deu jeito nesse processo e agora é hora de conferir o resultado. Vai lá, dá uma espiadinha aqui embaixo ióóóó ióóóó\\033[0m\")\n",
    "        result = spark.table(f\"{catalog_linked}.{name_schema}.{name_tabela}\").limit(2)\n",
    "        display(result)\n",
    "\n",
    "\n",
    "    ## COMANDO DISPLAY VIEW - AS PRIMEIRAS 2 LINHAS\n",
    "    def _display_view():\n",
    "        print(\"\\033[38;5;40m➥ A mula Diana deu jeito nesse processo e agora é hora de conferir o resultado. Vai lá, dá uma espiadinha aqui embaixo ióóóó ióóóó\\033[0m\")\n",
    "        result = spark.table(f\"{catalog_linked}.{name_schema}.{name_view}\").limit(2)\n",
    "        display(result)\n",
    "            \n",
    "\n",
    "    ## COMANDO PARA CHAMAR A MULINHA iiiiiiooooooo\n",
    "    def _mula_faceira_servico(): ## IMAGEM MULA\n",
    "        caminho_imagem = \"/dbfs/FileStore/rafael_rpereira/Mula_faceira.png\"\n",
    "        imagem_faceira = Image.open(caminho_imagem)\n",
    "        display(imagem_faceira)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "676d680b-40c4-4917-a875-6f2cb15efc0a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Execução do Serviço"
    }
   },
   "outputs": [],
   "source": [
    "class TableManager:\n",
    "\n",
    "    ##### TEMP VIEW TERADARA\n",
    "    def create_temp_view_from_tera(query, temp_view_name, is_global=False):\n",
    "        df = FrameConection().get_connect_teradata(query)\n",
    "        if is_global:\n",
    "            df.createOrReplaceGlobalTempView(temp_view_name)\n",
    "        else:\n",
    "            df.createOrReplaceTempView(temp_view_name)\n",
    "        print(\"\\033[38;5;40m➥ Temp view criada com sucesso via Teradata\\033[0m\")\n",
    "\n",
    "\n",
    "    ##### TEMP VIEW DENODO\n",
    "    def create_temp_view_from_denodo(query_dnd, temp_view_name, is_global=False):\n",
    "        df = FrameConection().databricks_denodo_connection(query_dnd)\n",
    "        if is_global:\n",
    "            df.createOrReplaceGlobalTempView(temp_view_name)\n",
    "        else:\n",
    "            df.createOrReplaceTempView(temp_view_name)\n",
    "        print(\"\\033[38;5;40m➥ Temp view criada com sucesso via Denodo\\033[0m\")\n",
    "\n",
    "\n",
    "    ##### TEMP VIEW REF/PYTHON\n",
    "    def create_temp_view(query, temp_view_name, is_global=False):\n",
    "        if is_global:\n",
    "            spark.sql(f\"CREATE OR REPLACE GLOBAL TEMPORARY VIEW {temp_view_name} AS {query}\")\n",
    "        else:\n",
    "            spark.sql(f\"CREATE OR REPLACE TEMPORARY VIEW {temp_view_name} AS {query}\")\n",
    "        print(\"\\033[38;5;40m➥ Temp view criada com sucesso via lake house Databricks\\033[0m\")\n",
    "\n",
    "\n",
    "    ##### CHECANDO QUALIDADE DA TABELA\n",
    "    def check_table():\n",
    "        print(\"\\033[94m\\n===>  Comando CHECK_TABLE iniciado...\\033[0m\")\n",
    "        ComandVariavel._atualiza_variavel()\n",
    "        ComandCheck._check_table()\n",
    "        \n",
    "\n",
    "    ##### DROPAR TABELA\n",
    "    def drop_table():\n",
    "        print(\"\\033[94m\\n===>  Comando DROP_TABLE iniciado...\\033[0m\")\n",
    "        ComandDrop._drop_table()\n",
    "        print(f\"\\033[38;5;40m➥ A tabela {catalog_linked}.{name_schema}.{name_tabela} e seus metadados foram apagados via comando sistêmico\")\n",
    "\n",
    "\n",
    "    ##### CRIAR TABELA - OVERWRITE\n",
    "    def create_table_overwrite():\n",
    "        if check_table_active is None:\n",
    "            print(\"\"\"\\033[94m\\n===>  Comando CHECK_TABLE não iniciado!!!\\n\\033[0m\\033[93m⚠ Atenção - O usuário optou por não validar este objeto antes de sua criação, a mula Diana não recomenda esta ação🫏\\033[0m\"\"\")\n",
    "        pass\n",
    "        print(\"\\033[94m\\n===>  Comando CREATE TABLE OVERWRITE iniciado...\\033[0m\")\n",
    "        ComandVariavel._atualiza_variavel()\n",
    "        ComandColumnsAdd._add_columns()\n",
    "        ComandCreateOver._create_over_table()\n",
    "        ComandMetadados._atualiza_metadados()\n",
    "        ComandMetadados._sync_unity_catalog()\n",
    "        ComandTime.calculed_time()\n",
    "\n",
    "\n",
    "    ##### CRIAR TABELA - APPEND\n",
    "    def create_table_append():\n",
    "        if check_table_active is None:\n",
    "            print(\"\"\"\\033[94m\\n===>  Comando CHECK_TABLE não iniciado!!!\\n\\033[0m\\033[93m⚠ Atenção - O usuário optou por não validar este objeto antes de sua criação, a mula Diana não recomenda esta ação🫏\\033[0m\"\"\")\n",
    "        pass\n",
    "        print(\"\\033[94m\\n===>  Comando CREATE TABLE APPEND iniciado...\\033[0m\")\n",
    "        ComandVariavel._atualiza_variavel()\n",
    "        ComandColumnsAdd._add_columns()\n",
    "        ComandCreateAppe._create_appe_table()\n",
    "        ComandMetadados._atualiza_metadados()\n",
    "        ComandMetadados._sync_unity_catalog()\n",
    "        ComandTime.calculed_time()\n",
    "\n",
    "\n",
    "    ##### CRIAR TABELAS - UPSERT\n",
    "    def create_table_upsert():\n",
    "        if check_table_active is None:\n",
    "            print(\"\"\"\\033[94m\\n===>  Comando CHECK_TABLE não iniciado!!!\\n\\033[0m\\033[93m⚠ Atenção - O usuário optou por não validar este objeto antes de sua criação, a mula Diana não recomenda esta ação🫏\\033[0m\"\"\")\n",
    "        pass\n",
    "        print(\"\\033[94m\\n===>  Comando CREATE TABLE UPSERT iniciado...\\033[0m\")\n",
    "        ComandVariavel._atualiza_variavel()\n",
    "        ComandColumnsAdd._add_columns()\n",
    "        ComandUpsert._create_upsert_table()\n",
    "        ComandMetadados._atualiza_metadados()\n",
    "        ComandMetadados._sync_unity_catalog()\n",
    "        ComandTime.calculed_time()\n",
    "\n",
    "\n",
    "    ##### CRIAR VIEW - OVERWRITE\n",
    "    def create_view():\n",
    "        print(\"\\033[94m\\n===>  Comando CREATE VIEW iniciado...\\033[0m\")\n",
    "        ComandVariavel._atualiza_view()\n",
    "        #ComandColumnsAdd._add_columns()\n",
    "        ComandCreateView._create_view_table()\n",
    "        ComandMetadados._sync_unity_catalog_view()\n",
    "\n",
    "\n",
    "    ##### DISPLAY TABLE - 2 LINHAS\n",
    "    def display_table():\n",
    "        print(\"\\033[94m\\n===>  Comando DISPLAY TABLE iniciado...\\033[0m\")\n",
    "        ComandVariavel._atualiza_variavel()\n",
    "        if environment == \"dev\":\n",
    "            ComandDisplay._mula_faceira_servico()\n",
    "        else:\n",
    "            pass\n",
    "        print(tipo_escrita)\n",
    "        ComandDisplay._display_table()\n",
    "\n",
    "\n",
    "    ##### DISPLAY VIEW - 2 LINHAS\n",
    "    def display_view():\n",
    "        print(\"\\033[94m\\n===>  Comando DISPLAY VIEW iniciado...\\033[0m\")\n",
    "        ComandVariavel._atualiza_view()\n",
    "        if environment == \"dev\":\n",
    "            ComandDisplay._mula_faceira_servico()\n",
    "        else:\n",
    "            pass\n",
    "        ComandDisplay._display_view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93a6652a-352b-4f66-a787-c87a1b4b076a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Impressões Diversas"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[94m#  LEGENDA\u001B[0m\n\u001B[32m    𒊹𒊹𒊹 Verde = ➥ Muito bom\u001B[0m\n\u001B[93m    𒊹𒊹𒊹 Amarelo = ⚠ Atenção\u001B[0m\n\u001B[91m    𒊹𒊹𒊹 Vermelho = ✖ Não recomendado\u001B[0m\n    𒊹𒊹𒊹 Branco = Dados sistêmicos\n\n\u001B[94m#  MODELOS DE SERVIÇO\u001B[0m\n    𒊹𒊹𒊹 Documentação do framework:\n           ⤷ https://banco_cooperativo-dev.cloud.databricks.com/?o=#workspace/Users/rafael_rpereira@banco_cooperativo.com.br/repositorio_exps/utils/Diana_framework/Documentação_Diana_framework\n    𒊹𒊹𒊹 Notebook modelo View:\n           ⤷ https://banco_cooperativo-dev.cloud.databricks.com/?o=#workspace/Users/rafael_rpereira@banco_cooperativo.com.br/repositorio_exps/utils/Diana_framework/Note_modelo_view\n    𒊹𒊹𒊹 Notebook modelo Tabela:\n           ⤷ https://banco_cooperativo-dev.cloud.databricks.com/?o=#workspace/Users/rafael_rpereira@banco_cooperativo.com.br/repositorio_exps/utils/Diana_framework/Note_modelo_tabela\n    𒊹𒊹𒊹 Notebook modelo Tabela (Python):\n           ⤷ https://banco_cooperativo-dev.cloud.databricks.com/?o=#workspace/Users/rafael_rpereira@banco_cooperativo.com.br/repositorio_exps/utils/Diana_framework/Note_modelo_tabela_python\n\n\u001B[94m#  CONFIGURAÇÃO DE AMBIENTE\u001B[0m\n    Catálogo selecioinado -> None\n    Versão Framework Diana -> Diana 1.5\n\u001B[32m\n➥ Framework Diana carregado com sucesso\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"\\033[94m#  LEGENDA\\033[0m\")\n",
    "print(f\"\\033[32m    𒊹𒊹𒊹 Verde = ➥ Muito bom\\033[0m\")\n",
    "print(f\"\\033[93m    𒊹𒊹𒊹 Amarelo = ⚠ Atenção\\033[0m\")\n",
    "print(f\"\\033[91m    𒊹𒊹𒊹 Vermelho = ✖ Não recomendado\\033[0m\")\n",
    "print(\"    𒊹𒊹𒊹 Branco = Dados sistêmicos\")\n",
    "print(\"\\n\\033[94m#  MODELOS DE SERVIÇO\\033[0m\")\n",
    "print(f\"    𒊹𒊹𒊹 Documentação do framework:\\n           ⤷ {note_documentacao}\")\n",
    "print(f\"    𒊹𒊹𒊹 Notebook modelo View:\\n           ⤷ {Note_modelo_view}\")\n",
    "print(f\"    𒊹𒊹𒊹 Notebook modelo Tabela:\\n           ⤷ {Note_modelo_tabela}\")\n",
    "print(f\"    𒊹𒊹𒊹 Notebook modelo Tabela (Python):\\n           ⤷ {Note_modelo_tabela_python}\")\n",
    "print(\"\\n\\033[94m#  CONFIGURAÇÃO DE AMBIENTE\\033[0m\")\n",
    "#print(f\"    Ambiente de serviço -> {environment}\")\n",
    "print(f\"    Catálogo selecioinado -> {catalog_service}\")\n",
    "#print(f\"    Bucket de salvamento -> {default_bucket}\")\n",
    "print(f\"    Versão Framework Diana -> {NUM_VERSION}\")\n",
    "print(f\"\\033[32m\\n➥ Framework Diana carregado com sucesso\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b581042-2045-4208-afa7-c02efc1762f2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Autor"
    }
   },
   "outputs": [],
   "source": [
    "# Data de criação 10/03/2025\n",
    "# Autores(\n",
    "#   rafael_rpereira@banco_cooperativo.com.br\n",
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4,
    "widgetLayout": []
   },
   "notebookName": "diana_framework",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
